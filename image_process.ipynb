{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T13:18:13.183594Z",
     "start_time": "2019-01-05T13:18:04.235294Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import *\n",
    "from skimage import exposure\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.dehaze import dehaze\n",
    "from utils.lee_filter import lee_filter\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import os\n",
    "import ast\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [16, 10]\n",
    "plt.rcParams['font.size'] = 14\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "root_dir = '../'\n",
    "feats_dir = os.path.join(root_dir,'feats')\n",
    "stage1_feats_dir = os.path.join(feats_dir,'stage1')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T13:18:13.435687Z",
     "start_time": "2019-01-05T13:18:13.188185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "test a part\n",
      "(4838, 32, 32, 8)\n",
      "(4838, 32, 32, 10)\n",
      "------------------------------------------------------------\n",
      "test b part\n",
      "(4835, 32, 32, 8)\n",
      "(4835, 32, 32, 10)\n",
      "------------------------------------------------------------\n",
      "training part\n",
      "(352366, 32, 32, 8)\n",
      "(352366, 32, 32, 10)\n",
      "(352366, 17)\n",
      "------------------------------------------------------------\n",
      "validation part\n",
      "(24119, 32, 32, 8)\n",
      "(24119, 32, 32, 10)\n",
      "(24119, 17)\n"
     ]
    }
   ],
   "source": [
    "### to change according to your machine\n",
    "data_dir = os.path.join(root_dir,'dataset')\n",
    "\n",
    "path_test_a = os.path.join(data_dir,'round1_test_a_20181109.h5')\n",
    "path_test_b = os.path.join(data_dir,'round1_test_b_20190104.h5')\n",
    "path_validation = os.path.join(data_dir,'validation.h5')\n",
    "path_training = os.path.join(data_dir,'training.h5')\n",
    "\n",
    "fid_test_a = h5py.File(path_test_a,'r')\n",
    "fid_test_b = h5py.File(path_test_b,'r')\n",
    "fid_validation = h5py.File(path_validation,'r')\n",
    "fid_training = h5py.File(path_training,'r')\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"test a part\")\n",
    "s1_test_a = fid_test_a['sen1']\n",
    "print(s1_test_a.shape)\n",
    "s2_test_a = fid_test_a['sen2']\n",
    "print(s2_test_a.shape)\n",
    "print(\"-\" * 60)\n",
    "print(\"test b part\")\n",
    "s1_test_b = fid_test_b['sen1']\n",
    "print(s1_test_b.shape)\n",
    "s2_test_b = fid_test_b['sen2']\n",
    "print(s2_test_b.shape)\n",
    "print(\"-\" * 60)\n",
    "print(\"training part\")\n",
    "s1_training = fid_training['sen1']\n",
    "print(s1_training.shape)\n",
    "s2_training = fid_training['sen2']\n",
    "print(s2_training.shape)\n",
    "label_training = fid_training['label']\n",
    "print(label_training.shape)\n",
    "print(\"-\" * 60)\n",
    "print(\"validation part\")\n",
    "s1_validation = fid_validation['sen1']\n",
    "print(s1_validation.shape)\n",
    "s2_validation = fid_validation['sen2']\n",
    "print(s2_validation.shape)\n",
    "label_validation = fid_validation['label']\n",
    "print(label_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T13:18:14.322729Z",
     "start_time": "2019-01-05T13:18:13.438106Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 256. 1254. 2353.  849.  757. 1906.  474. 3395. 1914.  860. 2287.  382.\n",
      " 1202. 2747.  202.  672. 2609.]\n"
     ]
    }
   ],
   "source": [
    "per_class_num = 5000 #3000\n",
    "# valid_label_id = np.argmax(label_validation,axis=-1)\n",
    "valid_label_id_df = pd.DataFrame(np.argmax(label_validation,axis=-1)).rename(index=str,columns={0:'label'})\n",
    "train_label_id_df = pd.DataFrame(np.argmax(label_training,axis=-1)).rename(index=str,columns={0:'label'})\n",
    "valid_label_val_cnt = np.sum(label_validation,axis=0)\n",
    "train_label_val_cnt = np.sum(label_training,axis=0)\n",
    "print(valid_label_val_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T13:18:14.365042Z",
     "start_time": "2019-01-05T13:18:14.326830Z"
    }
   },
   "outputs": [],
   "source": [
    "# img transform\n",
    "\n",
    "def imgs_transform(imgs):\n",
    "    t_imgs = np.zeros_like(imgs)\n",
    "    for i in tqdm(range(imgs.shape[0])):\n",
    "        t_imgs[i,:,:,8:] = imgs[i,:,:,8:]/2.8\n",
    "        t_imgs[i,:,:,(10,9,8)] = exposure.rescale_intensity(dehaze(t_imgs[i,:,:,(10,9,8)].transpose(1,2,0))).transpose(2,0,1)\n",
    "        for j in range(4):\n",
    "            t_imgs[i,:,:,j] = lee_filter(imgs[i,:,:,j])\n",
    "        for j in range(4,8):\n",
    "            t_imgs[i,:,:,j] = imgs[i,:,:,j]\n",
    "    return t_imgs\n",
    "\n",
    "merged_imgs_v0 = imgs_transform(merged_imgs)\n",
    "test_imgs = np.concatenate([s1_test,s2_test],axis=-1)\n",
    "test_imgs_v0 = imgs_transform(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "merged_training_imgs:[18c]\n",
    "(a)train+valid\n",
    "(b)s1+s2\n",
    "(c)\n",
    "    [0-3]:s1[0-3]:lee filter\n",
    "    [4-7]:s1[4-7]:lee filter\n",
    "    [9-10]:s2[0-2]:dehaze+rescale_intensity\n",
    "    [11-17]:s2[3-9]:dehaze+rescale_intensity\n",
    "\"\"\"\n",
    "data_save_dir = os.path.join(root_dir,'dataset')\n",
    "np.save(os.path.join(data_save_dir,'merged_training_imgs'),merged_imgs_v0)\n",
    "np.save(os.path.join(data_save_dir,'merged_training_labels'),merged_labels)\n",
    "np.save(os.path.join(data_save_dir,'test_imgs'),test_imgs_v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T13:18:14.509879Z",
     "start_time": "2019-01-05T13:18:14.368288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmerged_training_imgs_v1:[17c]\\n(a)train+valid\\n(b)s1+s2\\n(c)\\n    [0]:s1[0,1]:lee filter + square_sum_sqrt_merged + omn_exp\\n    [1]:s1[2,3]:lee filter + square_sum_sqrt_merged + omn_exp\\n    [2-3]:s1[4,5]:log10 + norm[0-1]\\n    [4-5]:s1[6,7]:sqrt + norm[0-1]\\n    [6]:s1[6,7]:square_sum_sqrt_log10_merged + norm[0-1]\\n    [7-9]:s2[0-2]:dehaze + rescale_intensity + norm[0-1]\\n    [10-16]:s2[3-9]:norm[0-1]\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "merged_training_imgs_v1:[17c]\n",
    "(a)train+valid\n",
    "(b)s1+s2\n",
    "(c)\n",
    "    [0]:s1[0,1]:lee filter + square_sum_sqrt_merged + omn_exp\n",
    "    [1]:s1[2,3]:lee filter + square_sum_sqrt_merged + omn_exp\n",
    "    [2-3]:s1[4,5]:log10 + norm[0-1]\n",
    "    [4-5]:s1[6,7]:sqrt + norm[0-1]\n",
    "    [6]:s1[6,7]:square_sum_sqrt_log10_merged + norm[0-1]\n",
    "    [7-9]:s2[0-2]:dehaze + rescale_intensity + norm[0-1]\n",
    "    [10-16]:s2[3-9]:norm[0-1]\n",
    "\"\"\"\n",
    "\n",
    "def get_min_max(data):\n",
    "    min_max = []\n",
    "    for i in range(data.shape[-1]):\n",
    "        min_max.append((np.min(data[:,:,:,i]),np.max(data[:,:,:,i])))\n",
    "    return min_max\n",
    "\n",
    "def norm_01(data,min_max):\n",
    "    min_val,max_val = min_max\n",
    "#     min_val = np.min(data)\n",
    "#     max_val = np.max(data)\n",
    "    return (data - min_val)/(max_val-min_val)\n",
    "\n",
    "def omn_exp(data):\n",
    "    return 1-np.exp(np.negative(data))\n",
    "\n",
    "def imgs_v1_transform(imgs):\n",
    "    imgs_v1 = np.zeros((*imgs.shape[:-1],17))\n",
    "    \n",
    "    imgs_v1[:,:,:,0] = omn_exp(np.sqrt(imgs[:,:,:,0]**2 + imgs[:,:,:,1]**2))\n",
    "    imgs_v1[:,:,:,1] = omn_exp(np.sqrt(imgs[:,:,:,2]**2 + imgs[:,:,:,3]**2))\n",
    "\n",
    "    imgs_v1[:,:,:,2] = np.log10(imgs[:,:,:,4])\n",
    "    imgs_v1[:,:,:,3] = np.log10(imgs[:,:,:,5])\n",
    "\n",
    "    imgs_v1[:,:,:,4] = np.sqrt(abs(imgs[:,:,:,6])) * np.where(imgs[:,:,:,6]>0,1,-1)\n",
    "    imgs_v1[:,:,:,5] = np.sqrt(abs(imgs[:,:,:,7])) * np.where(imgs[:,:,:,7]>0,1,-1)\n",
    "    imgs_v1[:,:,:,6] = np.log10(np.sqrt(imgs[:,:,:,6]**2 + imgs[:,:,:,7]**2))\n",
    "\n",
    "    imgs_v1[:,:,:,7:] = imgs[:,:,:,8:]\n",
    "    return imgs_v1\n",
    "\n",
    "def norm_imgs(imgs,min_max,norm_ind):\n",
    "    for i in norm_ind:\n",
    "        imgs[:,:,:,i] = norm_01(imgs[:,:,:,i],min_max[i])\n",
    "\n",
    "data_save_dir = os.path.join(root_dir,'dataset','merged_data','v1')\n",
    "merged_imgs_v1 = imgs_v1_transform(merged_imgs_v0)\n",
    "min_max = get_min_max(merged_imgs_v1)\n",
    "print(min_max)\n",
    "norm_imgs(merged_imgs_v1,min_max,[2,3,4,5,6])\n",
    "np.save(os.path.join(data_save_dir,'merged_training_imgs_v1'),merged_imgs_v1)\n",
    "np.save(os.path.join(data_save_dir,'merged_training_labels_v1'),merged_labels)\n",
    "print('process training imgs finished')\n",
    "test_imgs_v1 = imgs_v1_transform(test_imgs_v0)\n",
    "norm_imgs(test_imgs_v1,min_max,[2,3,4,5,6])\n",
    "np.save(os.path.join(data_save_dir,'test_imgs_v1'),test_imgs_v1)\n",
    "print('process testing imgs finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
